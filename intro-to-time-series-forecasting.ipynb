{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nIn this notebook, we will learn how to work with and predict time series. Time series are a collection of **time-dependent data** points. That means that each data point is assigned to a specific timestamp. Ideally, these data points are in chronological order and in contant time intervals (e.g. every minute or everyday). The time series forecasting problem **analyzes patterns in the past data to make predictions about the future**. The most popular example is probably stock price prediction. Other examples are sales of seasonal clothing or weather forecasts. In contrast to regression problems, time series are time-dependent and show specific characteristics, such as **trend and seasonality**.\n\n**Overview**\n* [Problem Definition](#Problem-Definition)<br>\n* [Data Collection](#Data-Collection)<br>\n* [Data Preprocessing](#Data-Preprocessing)<br>\n    * [Chronological Order and Equidistant Timestamps](#[Chronological-Order-and-Equidistant-Timestamps])<br>\n    * [Handling Missing Values](#Handling-Missing-Values)<br>\n    * [Resampling](#Resampling)<br>\n    * [Stationarity](#Stationarity)<br>\n* [Feature Engineering](#Feature-Engineering)<br>\n    * [Time Features](#Time-Features)<br>\n    * [Decomposition](#Decomposition)<br>\n    * [Lag](#Lag)<br>\n* [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n    * [Autocorrelation Analysis](#Autocorrelation-Analysis)<br> \n* [Cross Validation](#Cross-Validation)<br>\n* [Models](#Models)<br>\n    * [Models for Univariate Time Series](#Models-for-Univariate-Time-Series)<br>\n        * [Naive Approach](#Naive-Approach)<br>\n        * [Moving Average](#Moving-Average)<br>\n        * [Exponential Smoothing  (IN WORK)](#MExponential-Smoothing)<br>\n        * [ARIMA](#ARIMA)<br>\n    * [Models for Multivariate Time Series](#Models-for-Multivariate-Time-Series)<br>\n        * [Vector Autoregression (VAR)](#Vector-Autoregression)<br>","metadata":{}},{"cell_type":"markdown","source":"# Problem Definition\nFor this tutorial, we will build a model to predict the depth to groundwater of an aquifer located in Petrignano, Italy. The question we want to answer is\n> What is the future depth to groundwater of a well belonging to the aquifier in Petrigrano over the next quarter?\n\n> The wells field of the alluvial plain between Ospedalicchio di Bastia Umbra and Petrignano is fed by three underground aquifers separated by low permeability septa. The aquifer can be considered a water table groundwater and is also fed by the Chiascio river. The groundwater levels are influenced by the following parameters: rainfall, depth to groundwater, temperatures and drainage volumes, level of the Chiascio river.\n\n> Indeed, both rainfall and temperature affect features like level, flow, depth to groundwater and hydrometry some time after it fell down. \n\n# Data Collection \nIn a typical workflow for time series, this would be the time for data collection. In this example, we will skip the data collection step and use data from the [Acea Smart Water Analytics challenge](https://www.kaggle.com/c/acea-water-prediction/). Therefore, this section will be a dataset overview. \n\nAlthough the dataset contains multiple waterbodies, we will only be looking at the Aquifer_Petrignano.csv file.\n\nTime series data usually comes in **tabular** format (e.g. csv files).","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns # Visualization\nimport matplotlib.pyplot as plt # Visualization\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\nimport warnings # Supress warnings \nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"../input/acea-water-prediction/Aquifer_Petrignano.csv\")\n\n### Simplifications for the sake of the tutorial ###\n# Drop data before 2009 for the purpose of this tutorial\ndf = df[df.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\n# Drop one of the target columns, so we can focus on only one target\ndf = df.drop(['Depth_to_Groundwater_P24', 'Temperature_Petrignano'], axis=1)\n\n# Simplify column names\ndf.columns = ['Date', 'Rainfall', 'Depth_to_Groundwater', 'Temperature', 'Drainage_Volume', 'River_Hydrometry']\n\ntargets = ['Depth_to_Groundwater']\nfeatures = [feature for feature in df.columns if feature not in targets]\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are working with time series, the most essential features are the time related feature. In this example, we have the column `Date` which  uniquely identifies a day. Ideally, the data is already in chronological order and the time stamps are equidistant in time series. This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. \n\n\nThis column is provided in string format. Let's convert it to the `datetime64[ns]` data type.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, date \n\ndf['Date'] = pd.to_datetime(df.Date, format = '%d/%m/%Y')\ndf.head().style.set_properties(subset=['Date'], **{'background-color': 'dodgerblue'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features:\n* **Rainfall** indicates the quantity of rain falling (mm)\n* **Temperature** indicates the temperature (Â°C) \n* **Volume** indicates the volume of water taken from the drinking water treatment plant (m$^3$)\n* **Hydrometry** indicates the groundwater level (m)\n\nTarget:\n* **Depth to Groundwater** indicates the groundwater level (m from the ground floor)\n","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 25))\n\nsns.lineplot(x=df.Date, y=df.Rainfall.fillna(np.inf), ax=ax[0], color='dodgerblue')\nax[0].set_title('Feature: Rainfall', fontsize=14)\nax[0].set_ylabel(ylabel='Rainfall', fontsize=14)\n\n\nsns.lineplot(x=df.Date, y=df.Temperature.fillna(np.inf), ax=ax[1], color='dodgerblue',label='Bastia Umbra')\nax[1].set_title('Feature: Temperature', fontsize=14)\nax[1].set_ylabel(ylabel='Temperature', fontsize=14)\n\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[2], color='dodgerblue')\nax[2].set_title('Feature: Volume', fontsize=14)\nax[2].set_ylabel(ylabel='Volume', fontsize=14)\n\n\nsns.lineplot(x=df.Date, y=df.River_Hydrometry.fillna(np.inf), ax=ax[3], color='dodgerblue')\nax[3].set_title('Feature: Hydrometry', fontsize=14)\nax[3].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\n\nsns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.fillna(np.inf), ax=ax[4], color='dodgerblue')\nax[4].set_title('Target: Depth to Groundwater', fontsize=14)\nax[4].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\n\nfor i in range(5):\n    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\n## Chronological Order and Equidistant Timestamps\nThe data should be in **chronological order** and the **timestamps should be equidistant** in time series. The chronological order can be achieved by sorting the dataframe by the timestamps. Equidisant timestamps indicates constant time intervals. To check this, the difference between each timestamp can be taken. If this is not the case, you can decide on a constant time interval and resample the data (see [Resampling](#Resampling)).\n\nThis is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step. ","metadata":{}},{"cell_type":"code","source":"# Sort values by timestamp (not necessary in this case)\ndf = df.sort_values(by='Date')\n\n# Check time intervals\ndf['Time_Interval'] = df.Date - df.Date.shift(1)\n\ndf[['Date', 'Time_Interval']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{df['Time_Interval'].value_counts()}\")\ndf = df.drop('Time_Interval', axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Missing Values\n\nWe can see that `Depth_to_Groundwater` has missing values.\n\nFurthermore, plotting the time series reveals that there seem to be some **implausible zero values** for `Drainage_Volume`, and `River_Hydrometry`. We will have to clean them by replacing them by `nan` values and filling them afterwards.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\nold = df.River_Hydrometry.copy()\ndf['River_Hydrometry'] = np.where((df.River_Hydrometry == 0),np.nan, df.River_Hydrometry)\n\nsns.lineplot(x=df.Date, y=old.fillna(np.inf), ax=ax[0], color='darkorange', label = 'original')\nsns.lineplot(x=df.Date, y=df.River_Hydrometry.fillna(np.inf), ax=ax[0], color='dodgerblue', label = 'modified')\nax[0].set_title('Feature: Hydrometry', fontsize=14)\nax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\nold = df.Drainage_Volume.copy()\ndf['Drainage_Volume'] = np.where((df.Drainage_Volume == 0),np.nan, df.Drainage_Volume)\n\nsns.lineplot(x=df.Date, y=old.fillna(np.inf), ax=ax[1], color='darkorange', label = 'original')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[1], color='dodgerblue', label = 'modified')\nax[1].set_title('Feature: Volume', fontsize=14)\nax[1].set_ylabel(ylabel='Volume', fontsize=14)\n\n\nfor i in range(2):\n    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to think about what to do with these missing values. ","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(df.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Option 1: Fill NaN with Outlier or Zero**\n\n    In this specific example filling the missing value with an outlier value such as -999 is not a good idea. However, many notebooks in this challenge have been using -999. \n    \n* **Option 2: Fill NaN with Mean Value**\n\n    Also in this example, we can see that filling NaNs with the mean value is also not sufficient.\n\n* **Option 3: Fill NaN with Last Value with `.ffill()`**\n\n    Filling NaNs with the last value is already a little bit better in this case.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with `.interpolate()`**\n\n    Filling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.\n","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4, ncols=1, figsize=(15, 12))\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(0), ax=ax[0], color='darkorange', label = 'modified')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[0], color='dodgerblue', label = 'original')\nax[0].set_title('Fill NaN with 0', fontsize=14)\nax[0].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nmean_val = df.Drainage_Volume.mean()\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(mean_val), ax=ax[1], color='darkorange', label = 'modified')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[1], color='dodgerblue', label = 'original')\nax[1].set_title(f'Fill NaN with Mean Value ({mean_val:.0f})', fontsize=14)\nax[1].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.ffill(), ax=ax[2], color='darkorange', label = 'modified')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[2], color='dodgerblue', label = 'original')\nax[2].set_title(f'FFill', fontsize=14)\nax[2].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.interpolate(), ax=ax[3], color='darkorange', label = 'modified')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.fillna(np.inf), ax=ax[3], color='dodgerblue', label = 'original')\nax[3].set_title(f'Interpolate', fontsize=14)\nax[3].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nfor i in range(4):\n    ax[i].set_xlim([date(2019, 5, 1), date(2019, 10, 1)])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Drainage_Volume'] = df['Drainage_Volume'].interpolate()\ndf['River_Hydrometry'] = df['River_Hydrometry'].interpolate()\ndf['Depth_to_Groundwater'] = df['Depth_to_Groundwater'].interpolate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resampling\n\nResampling can provide additional information on the data. There are two types of resampling:\n* **Upsampling** is when the frequency of samples is increased (e.g. days to hours)\n* **Downsampling** is when the frequency of samples is decreased (e.g. days to weeks)\n\nIn this example, we will do some downsampling with the `.resample()` function.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,12))\n\nax[0, 0].bar(df.Date, df.Rainfall, width=5, color='dodgerblue')\nax[0, 0].set_title('Daily Rainfall (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Rainfall']].resample('7D', on='Date').sum().reset_index(drop=False)\nax[1, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=10, color='dodgerblue')\nax[1, 0].set_title('Weekly Rainfall (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Rainfall']].resample('M', on='Date').sum().reset_index(drop=False)\nax[2, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=15, color='dodgerblue')\nax[2, 0].set_title('Monthly Rainfall (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Rainfall']].resample('12M', on='Date').sum().reset_index(drop=False)\nax[3, 0].bar(resampled_df.Date, resampled_df.Rainfall, width=20, color='dodgerblue')\nax[3, 0].set_title('Annual Rainfall (Acc.)', fontsize=14)\n\nfor i in range(4):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(df.Date, df.Temperature, color='dodgerblue', ax=ax[0, 1])\nax[0, 1].set_title('Daily Temperature (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Temperature']].resample('7D', on='Date').mean().reset_index(drop=False)\nsns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[1, 1])\nax[1, 1].set_title('Weekly Temperature (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Temperature']].resample('M', on='Date').mean().reset_index(drop=False)\nsns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[2, 1])\nax[2, 1].set_title('Monthly Temperature (Acc.)', fontsize=14)\n\nresampled_df = df[['Date','Temperature']].resample('365D', on='Date').mean().reset_index(drop=False)\nsns.lineplot(resampled_df.Date, resampled_df.Temperature, color='dodgerblue', ax=ax[3, 1])\nax[3, 1].set_title('Annual Temperature (Acc.)', fontsize=14)\n\nfor i in range(4):\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    ax[i, 1].set_ylim([-5, 35])\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this example, resampling would not be necessary. On the other hand, there is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. Therefore, we will **downsample the data to a weekly basis**.","metadata":{}},{"cell_type":"code","source":"df_downsampled = df[['Date',\n                     'Depth_to_Groundwater', \n                     'Temperature',\n                     'Drainage_Volume', \n                     'River_Hydrometry'\n                    ]].resample('7D', on='Date').mean().reset_index(drop=False)\n\ndf_downsampled['Rainfall'] = df[['Date',\n                                 'Rainfall'\n                                ]].resample('7D', on='Date').sum().reset_index(drop=False)[['Rainfall']]\n\ndf = df_downsampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stationarity\n\nSome time-series models, such as such as [ARIMA](#ARIMA), assume that the underlying data is stationary. \nStationarity describes that the time-series has\n* constant mean and mean is not time-dependent \n* constant variance and variance is not time-dependent \n* constant covariance and covariance is not time-dependent \n\n> If a time series has a specific (stationary) behavior over a given time interval, then it can be assumed that the time series will behave the same at a later time.\n\nTime series **with trend and/or seasonality are not stationary**. Trend indicates that the mean is not constant over time and seasonality indicates that the variance is not constant over time.","metadata":{}},{"cell_type":"code","source":"t = np.linspace(0, 19, 20)\n\nfig, ax = plt.subplots(ncols=4, nrows=1, figsize=(20,4))\nstationary = [5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6, 5, 4, 5, 6,]\nsns.lineplot(x=t, y=stationary, ax=ax[0], color='forestgreen')\nsns.lineplot(x=t, y=5, ax=ax[0], color='grey')\nsns.lineplot(x=t, y=6, ax=ax[0], color='grey')\nsns.lineplot(x=t, y=4, ax=ax[0], color='grey')\nax[0].lines[2].set_linestyle(\"--\")\nax[0].lines[3].set_linestyle(\"--\")\nax[0].set_title(f'Stationary \\nconstant mean \\nconstant variance \\nconstant covariance', fontsize=14)\n\nnonstationary1 = [ 9, 0, 1, 10, 8, 1, 2, 9, 7, 2, 3, 8, 6, 3, 4, 7, 5, 4, 5, 6]\nsns.lineplot(x=t, y=nonstationary1, ax=ax[1], color='indianred' )\nsns.lineplot(x=t, y=5, ax=ax[1], color='grey')\nsns.lineplot(x=t, y=t*0.25-0.5, ax=ax[1], color='grey')\nsns.lineplot(x=t, y=t*(-0.25)+11, ax=ax[1], color='grey')\nax[1].lines[2].set_linestyle(\"--\")\nax[1].lines[3].set_linestyle(\"--\")\nax[1].set_title(f'Non Stationary \\nconstant mean \\n non-constant variance\\nnconstant covariance', fontsize=14)\n\nnonstationary2 = [0, 2, 1, 3, 2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10, 9, 11,]\nsns.lineplot(x=t, y=nonstationary2, ax=ax[2], color='indianred' )\nsns.lineplot(x=t, y=t*0.5+0.7, ax=ax[2], color='grey')\nsns.lineplot(x=t, y=t*0.5, ax=ax[2], color='grey')\nsns.lineplot(x=t, y=t*0.5+1.5, ax=ax[2], color='grey')\nax[2].lines[2].set_linestyle(\"--\")\nax[2].lines[3].set_linestyle(\"--\")\nax[2].set_title(f'Non Stationary \\n non-constant mean\\nconstant variance\\nnconstant covariance', fontsize=14)\n\nnonstationary3 = [5, 4.5, 4, 4.5, 5, 5.5, 6, 5.5, 5, 4.5, 4, 5, 6, 5, 4, 6, 4, 6, 4, 6,]\nsns.lineplot(x=t, y=nonstationary3, ax=ax[3], color='indianred')\nsns.lineplot(x=t, y=5, ax=ax[3], color='grey')\nsns.lineplot(x=t, y=6, ax=ax[3], color='grey')\nsns.lineplot(x=t, y=4, ax=ax[3], color='grey')\nax[3].lines[2].set_linestyle(\"--\")\nax[3].lines[3].set_linestyle(\"--\")\nax[3].set_title(f'Stationary \\nconstant mean \\nconstant variance \\nnon-constant covariance', fontsize=14)\n\nfor i in range(4):\n    ax[i].set_ylim([-1, 12])\n    ax[i].set_xlabel('Time', fontsize=14)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The check for stationarity can be done via three different approaches:\n1. **visually**: plot time series and check for trends or seasonality\n2. **basic statistics**: split time series and compare the mean and variance of each partition\n3. **statistical test**: Augmented Dickey Fuller test\n\nLet's do the **visual check** first. We can see that all features except `Temperature` have non-constant mean and non-constant variance. Therefore, **none of these seem to be stationary**. However, `Temperature` shows strong seasonality (hot in summer, cold in winter) and therefore it is not stationary either.","metadata":{}},{"cell_type":"code","source":"rolling_window = 52\nf, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 12))\n\nsns.lineplot(x=df.Date, y=df.Rainfall, ax=ax[0, 0], color='indianred')\nsns.lineplot(x=df.Date, y=df.Rainfall.rolling(rolling_window).mean(), ax=ax[0, 0], color='black', label='rolling mean')\nsns.lineplot(x=df.Date, y=df.Rainfall.rolling(rolling_window).std(), ax=ax[0, 0], color='blue', label='rolling std')\nax[0, 0].set_title('Rainfall: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[0, 0].set_ylabel(ylabel='Rainfall', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Temperature, ax=ax[1, 0], color='indianred')\nsns.lineplot(x=df.Date, y=df.Temperature.rolling(rolling_window).mean(), ax=ax[1, 0], color='black', label='rolling mean')\nsns.lineplot(x=df.Date, y=df.Temperature.rolling(rolling_window).std(), ax=ax[1, 0], color='blue', label='rolling std')\nax[1, 0].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\nax[1, 0].set_ylabel(ylabel='Temperature', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.River_Hydrometry, ax=ax[0, 1], color='indianred')\nsns.lineplot(x=df.Date, y=df.River_Hydrometry.rolling(rolling_window).mean(), ax=ax[0, 1], color='black', label='rolling mean')\nsns.lineplot(x=df.Date, y=df.River_Hydrometry.rolling(rolling_window).std(), ax=ax[0, 1], color='blue', label='rolling std')\nax[0, 1].set_title('Hydrometry: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[0, 1].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume, ax=ax[1, 1], color='indianred')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.rolling(rolling_window).mean(), ax=ax[1, 1], color='black', label='rolling mean')\nsns.lineplot(x=df.Date, y=df.Drainage_Volume.rolling(rolling_window).std(), ax=ax[1, 1], color='blue', label='rolling std')\nax[1, 1].set_title('Volume: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[1, 1].set_ylabel(ylabel='Volume', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Depth_to_Groundwater, ax=ax[2, 0], color='indianred')\nsns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.rolling(rolling_window).mean(), ax=ax[2, 0], color='black', label='rolling mean')\nsns.lineplot(x=df.Date, y=df.Depth_to_Groundwater.rolling(rolling_window).std(), ax=ax[2, 0], color='blue', label='rolling std')\nax[2, 0].set_title('Depth to Groundwater: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[2, 0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\n\nfor i in range(3):\n    ax[i,0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    ax[i,1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will **check the underlying statistics**. For this we will **split the time series into two sections** and check the mean and the variance. You could do more partitions if you wanted.\n\nWith this method, `Temperature` and `River_Hydrometry` show **somewhat similar (constant) mean and variance** and could be seen as stationary. However, with this method, we are not able to see the seasonality in the `Temperature` feature.","metadata":{}},{"cell_type":"code","source":"num_partitions = 2\npartition_length = int(len(df) / num_partitions)\n\npartition1_mean = df.head(partition_length).mean()\npartition1_var = df.head(partition_length).var()\npartition2_mean = df.tail(partition_length).mean()\npartition2_var = df.tail(partition_length).var()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stationarity_test = pd.concat([partition1_mean, partition2_mean, partition1_var, partition2_var], axis=1)\nstationarity_test.columns = ['Partition 1 Mean', 'Partition 2 Mean', 'Partition 1 Variance', 'Partition 2 Variance']\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition 1 Mean'] == temp['Partition 2 Mean'])\n    m2 = (temp['Partition 1 Variance'] == temp['Partition 2 Variance'])\n    m3 = (temp['Partition 1 Mean'] < temp['Partition 2 Mean']+3) & (temp['Partition 1 Mean'] > temp['Partition 2 Mean']-3)\n    m4 = (temp['Partition 1 Variance'] < temp['Partition 2 Variance']+3) & (temp['Partition 1 Variance'] > temp['Partition 2 Variance']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition 1 Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'), df1['Partition 1 Mean'])\n    df1['Partition 2 Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'), df1['Partition 2 Mean'])\n    df1['Partition 1 Mean'] = np.where(m3, 'background-color: {}'.format('gold'), df1['Partition 1 Mean'])\n    df1['Partition 2 Mean'] = np.where(m3, 'background-color: {}'.format('gold'), df1['Partition 2 Mean'])\n    df1['Partition 1 Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition 1 Mean'])\n    df1['Partition 2 Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition 2 Mean'])\n\n    df1['Partition 1 Variance'] = np.where(~m2, 'background-color: {}'.format('salmon'), df1['Partition 1 Variance'])\n    df1['Partition 2 Variance'] = np.where(~m2, 'background-color: {}'.format('salmon'), df1['Partition 2 Variance'])\n    df1['Partition 1 Variance'] = np.where(m4, 'background-color: {}'.format('gold'), df1['Partition 1 Variance'])\n    df1['Partition 2 Variance'] = np.where(m4, 'background-color: {}'.format('gold'), df1['Partition 2 Variance'])\n    df1['Partition 1 Variance'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition 1 Variance'])\n    df1['Partition 2 Variance'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition 2 Variance'])\n\n    return df1\n\n\nstationarity_test.style.apply(highlight_greater, axis=None).format(\"{:20,.0f}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate the histograms. Since we are looking at the mean and variance, we are expecting that the data conforms to a Gaussian distribution (bell shaped distribution) in case of stationarity.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n\nsns.distplot(df.Rainfall.fillna(np.inf), ax=ax[0, 0], color='indianred')\nax[0, 0].set_title('Rainfall: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[0, 0].set_ylabel(ylabel='Rainfall', fontsize=14)\n\nsns.distplot(df.Temperature.fillna(np.inf), ax=ax[1, 0], color='indianred')\nax[1, 0].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\nax[1, 0].set_ylabel(ylabel='Temperature', fontsize=14)\n\nsns.distplot(df.River_Hydrometry.fillna(np.inf), ax=ax[0, 1], color='indianred')\nax[0, 1].set_title('Hydrometry: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[0, 1].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\nsns.distplot(df.Drainage_Volume.fillna(np.inf), ax=ax[1, 1], color='indianred')\nax[1, 1].set_title('Volume: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[1, 1].set_ylabel(ylabel='Volume', fontsize=14)\n\nsns.distplot(df.Depth_to_Groundwater.fillna(np.inf), ax=ax[2, 0], color='indianred')\nax[2, 0].set_title('Depth to Groundwater: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[2, 0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Augmented Dickey-Fuller (ADF) test**  is a type of statistical test called a unit root test.  Unit roots are a cause for non-stationarity.\n\n* **Null Hypothesis (H0)**: Time series has a unit root. (Time series is **not stationary**).\n\n* **Alternate Hypothesis (H1)**: Time series has no unit root (Time series is **stationary**).\n\nIf the **null hypothesis can be rejected**, we can conclude that the **time series is stationary**.\n\nThere are two ways to rejects the null hypothesis:\n\nOn the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n\n* <font color='red'>**p-value > significance level (default: 0.05)**</font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary</font>.\n* <font color='green'>**p-value <= significance level (default: 0.05)**</font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary</font>.\n    \nOn the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n* <font color='red'>**ADF statistic > critical value**</font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary</font>.\n* <font color='green'>**ADF statistic < critical value**</font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary</font>.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(df.Depth_to_Groundwater.values)\nadf_stat = result[0]\np_val = result[1]\ncrit_val_1 = result[4]['1%']\ncrit_val_5 = result[4]['5%']\ncrit_val_10 = result[4]['10%']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nf, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n\ndef visualize_adfuller_results(series, title, ax):\n    result = adfuller(series)\n    significance_level = 0.05\n    adf_stat = result[0]\n    p_val = result[1]\n    crit_val_1 = result[4]['1%']\n    crit_val_5 = result[4]['5%']\n    crit_val_10 = result[4]['10%']\n\n    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n        linecolor = 'forestgreen' \n    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n        linecolor = 'gold'\n    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n        linecolor = 'orange'\n    else:\n        linecolor = 'indianred'\n    sns.lineplot(x=df.Date, y=series, ax=ax, color=linecolor)\n    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n    ax.set_ylabel(ylabel=title, fontsize=14)\n\nvisualize_adfuller_results(df.Rainfall.values, 'Rainfall', ax[0, 0])\nvisualize_adfuller_results(df.Temperature.values, 'Temperature', ax[1, 0])\nvisualize_adfuller_results(df.River_Hydrometry.values, 'River_Hydrometry', ax[0, 1])\nvisualize_adfuller_results(df.Drainage_Volume.values, 'Drainage_Volume', ax[1, 1])\nvisualize_adfuller_results(df.Depth_to_Groundwater.values, 'Depth_to_Groundwater', ax[2, 0])\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the data is not stationary but we want to use a model that requires with characteristic, the data has to be transformed. However, if the data is not stationary to begin with, we should rethink the choice of model.\n\nThe two most common methods to achieve stationarity are:\n* **Transformation**: e.g. log or square root to stabilize non-constant variance\n* **Differencing**: subtracts the current value from the previous \n","metadata":{}},{"cell_type":"code","source":"# Log Transform of absolute values\n# (Log transoform of negative values will return NaN)\ndf['Depth_to_Groundwater_log'] = np.log(abs(df.Depth_to_Groundwater))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\nvisualize_adfuller_results(abs(df.Depth_to_Groundwater), 'Absolute \\n Depth to Groundwater', ax[0, 0])\n\nsns.distplot(df.Depth_to_Groundwater_log, ax=ax[0, 1])\nvisualize_adfuller_results(df.Depth_to_Groundwater_log, 'Transformed \\n Depth to Groundwater', ax[1, 0])\n\nsns.distplot(df.Depth_to_Groundwater_log, ax=ax[1, 1])\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Differencing can be done in different orders:\n* First order differencing: linear trends with $z_i = y_i - y_{i-1}$\n* Second-order differencing: quadratic trends with $z_i = (y_i - y_{i-1}) - (y_{i-1} - y_{i-2})$\n* and so on...","metadata":{}},{"cell_type":"code","source":"# First Order Differencing\nts_diff = np.diff(df.Depth_to_Groundwater)\ndf['Depth_to_Groundwater_diff_1'] = np.append([0], ts_diff)\n\n# Second Order Differencing\nts_diff = np.diff(df.Depth_to_Groundwater_diff_1)\ndf['Depth_to_Groundwater_diff_2'] = np.append([0], ts_diff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 6))\n\nvisualize_adfuller_results(df.Depth_to_Groundwater_diff_1, 'Differenced (1. Order) \\n Depth to Groundwater', ax[0])\nvisualize_adfuller_results(df.Depth_to_Groundwater_diff_2, 'Differenced (2. Order) \\n Depth to Groundwater', ax[1])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The differencing can be reverted if the the first value before differencing is known. In this case, we can accumulate all values with the function `.cumsum()` and add the first value of the original time series.","metadata":{}},{"cell_type":"code","source":"df.Depth_to_Groundwater.equals(df.Depth_to_Groundwater_diff_1.cumsum() + df.Depth_to_Groundwater.iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\n## Time Features","metadata":{}},{"cell_type":"code","source":"df['year'] = pd.DatetimeIndex(df['Date']).year\ndf['month'] = pd.DatetimeIndex(df['Date']).month\ndf['day'] = pd.DatetimeIndex(df['Date']).day\ndf['day_of_year'] = pd.DatetimeIndex(df['Date']).dayofyear\ndf['week_of_year'] = pd.DatetimeIndex(df['Date']).weekofyear\ndf['quarter'] = pd.DatetimeIndex(df['Date']).quarter\ndf['season'] = df.month%12 // 3 + 1\n\ndf[['Date', 'year', 'month', 'day', 'day_of_year', 'week_of_year', 'quarter', 'season']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Cyclical Features \nThe new time features are cyclical. For example,the feature `month` cycles between 1 and 12 for every year.\nWhile the difference between each month increments by 1 during the year, between two years the `month` feature jumps from 12 (December) to 1 (January). This results in a -11 difference, which can confuse a lot of models.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n\nsns.lineplot(x=df.Date, y=df.month, color='dodgerblue')\nax.set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ideally, we want the underlying data to represent the same difference between two consecutive months, even between December and January. A common remedy for this issue is to encode cyclical features into two dimensions with sine and cosine transformation.","metadata":{}},{"cell_type":"code","source":"month_in_year = 12\ndf['month_sin'] = np.sin(2*np.pi*df.month/month_in_year)\ndf['month_cos'] = np.cos(2*np.pi*df.month/month_in_year)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days_in_month = 30\ndf['day_sin'] = np.sin(2*np.pi*df.day/days_in_month)\ndf['day_cos'] = np.cos(2*np.pi*df.day/days_in_month)\n\ndays_in_year = 365\ndf['day_of_year_sin'] = np.sin(2*np.pi*df.day_of_year/days_in_year)\ndf['day_of_year_cos'] = np.cos(2*np.pi*df.day_of_year/days_in_year)\n\nweeks_in_year = 52.1429\ndf['week_of_year_sin'] = np.sin(2*np.pi*df.week_of_year/weeks_in_year)\ndf['week_of_year_cos'] = np.cos(2*np.pi*df.week_of_year/weeks_in_year)\n\nquarters_in_year = 4\ndf['quarter_sin'] = np.sin(2*np.pi*df.quarter/quarters_in_year)\ndf['quarter_cos'] = np.cos(2*np.pi*df.quarter/quarters_in_year)\n\nseasons_in_year = 4\ndf['season_sin'] = np.sin(2*np.pi*df.season/seasons_in_year)\ndf['season_cos'] = np.cos(2*np.pi*df.season/seasons_in_year)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\nsns.scatterplot(x=df.month_sin, y=df.month_cos, color='dodgerblue')\nplt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decomposition\n\nThe **characteristics of a time series** are\n* Trend and Level\n* Seasonality\n* Random / Noise\n\nWe can use the function `seasonal_decompose()` from the [statsmodels](https://www.statsmodels.org) library.\n\n* Additive: $y(t) = Level + Trend + Seasonality + Noise$\n* Multiplicative: $y(t) = Level * Trend * Seasonality * Noise$","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecompose_cols =  ['Rainfall', 'Temperature', \n                   'Drainage_Volume', \n                   'River_Hydrometry', 'Depth_to_Groundwater']\n\nfor col in decompose_cols:\n    decomp = seasonal_decompose(df[col], freq=52, model='additive', extrapolate_trend='freq')\n    df[f\"{col}_trend\"] = decomp.trend\n    df[f\"{col}_seasonal\"] = decomp.seasonal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,8))\nres = seasonal_decompose(df.Temperature, freq=52, model='additive', extrapolate_trend='freq')\n\nax[0,0].set_title('Decomposition of Temperature', fontsize=16)\nres.observed.plot(ax=ax[0,0], legend=False, color='dodgerblue')\nax[0,0].set_ylabel('Observed', fontsize=14)\nax[0,0].set_ylim([-5, 35])\n\nres.trend.plot(ax=ax[1,0], legend=False, color='dodgerblue')\nax[1,0].set_ylabel('Trend', fontsize=14)\nax[1,0].set_ylim([-5, 35])\n\nres.seasonal.plot(ax=ax[2,0], legend=False, color='dodgerblue')\nax[2,0].set_ylabel('Seasonal', fontsize=14)\nax[2,0].set_ylim([-15, 15])\n\nres.resid.plot(ax=ax[3,0], legend=False, color='dodgerblue')\nax[3,0].set_ylabel('Residual', fontsize=14)\nax[3,0].set_ylim([-15, 15])\n\nax[0,1].set_title('Decomposition of Depth_to_Groundwater', fontsize=16)\nres = seasonal_decompose(df.Depth_to_Groundwater, freq=52, model='additive', extrapolate_trend='freq')\n\nres.observed.plot(ax=ax[0, 1], legend=False, color='dodgerblue')\nax[0, 1].set_ylabel('Observed', fontsize=14)\n\nres.trend.plot(ax=ax[1, 1], legend=False, color='dodgerblue')\nax[1, 1].set_ylabel('Trend', fontsize=14)\n\nres.seasonal.plot(ax=ax[2, 1], legend=False, color='dodgerblue')\nax[2, 1].set_ylabel('Seasonal', fontsize=14)\n\nres.resid.plot(ax=ax[3, 1], legend=False, color='dodgerblue')\nax[3, 1].set_ylabel('Residual', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['Rainfall', 'Rainfall_trend', 'Rainfall_seasonal', \n          'Temperature', 'Temperature_trend', 'Temperature_seasonal', \n          'Drainage_Volume', 'Drainage_Volume_trend', 'Drainage_Volume_seasonal',\n          'River_Hydrometry', 'River_Hydrometry_trend', 'River_Hydrometry_seasonal', \n          'Depth_to_Groundwater', 'Depth_to_Groundwater_trend', 'Depth_to_Groundwater_seasonal']].head()\\\n.style.set_properties(subset=['Rainfall_trend', 'Rainfall_seasonal', \n                              'Temperature_trend', 'Temperature_seasonal', \n                              'Drainage_Volume_trend', 'Drainage_Volume_seasonal', \n                              'River_Hydrometry_trend', 'River_Hydrometry_seasonal',\n                              'Depth_to_Groundwater_trend', 'Depth_to_Groundwater_seasonal'\n                             ], **{'background-color': 'dodgerblue'})","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lag\n`.shift()`\n\n","metadata":{}},{"cell_type":"code","source":"weeks_in_month = 4\n\ndf['Temperature_seasonal_shift_r_2M'] = df.Temperature_seasonal.shift(-2*weeks_in_month)\ndf['Temperature_seasonal_shift_r_1M'] = df.Temperature_seasonal.shift(-1*weeks_in_month)\ndf['Temperature_seasonal_shift_1M'] = df.Temperature_seasonal.shift(1*weeks_in_month)\ndf['Temperature_seasonal_shift_2M'] = df.Temperature_seasonal.shift(2*weeks_in_month)\ndf['Temperature_seasonal_shift_3M'] = df.Temperature_seasonal.shift(3*weeks_in_month)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Drainage_Volume_seasonal_shift_r_2M'] = df.Drainage_Volume_seasonal.shift(-2*weeks_in_month)\ndf['Drainage_Volume_seasonal_shift_r_1M'] = df.Drainage_Volume_seasonal.shift(-1*weeks_in_month)\ndf['Drainage_Volume_seasonal_shift_1M'] = df.Drainage_Volume_seasonal.shift(1*weeks_in_month)\ndf['Drainage_Volume_seasonal_shift_2M'] = df.Drainage_Volume_seasonal.shift(2*weeks_in_month)\ndf['Drainage_Volume_seasonal_shift_3M'] = df.Drainage_Volume_seasonal.shift(3*weeks_in_month)\n\ndf['River_Hydrometry_seasonal_shift_r_2M'] = df.River_Hydrometry_seasonal.shift(-2*weeks_in_month)\ndf['River_Hydrometry_seasonal_shift_r_1M'] = df.River_Hydrometry_seasonal.shift(-1*weeks_in_month)\ndf['River_Hydrometry_seasonal_shift_1M'] = df.River_Hydrometry_seasonal.shift(1*weeks_in_month)\ndf['River_Hydrometry_seasonal_shift_2M'] = df.River_Hydrometry_seasonal.shift(2*weeks_in_month)\ndf['River_Hydrometry_seasonal_shift_3M'] = df.River_Hydrometry_seasonal.shift(3*weeks_in_month)\n\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(16,4))\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_r_2M, label='shifted by -2 month', ax=ax, color='lightblue')\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_r_1M, label='shifted by -1 month', ax=ax, color='skyblue')\n\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal, label='original', ax=ax, color='darkorange')\n\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_1M, label='shifted by 1 month', ax=ax, color='dodgerblue')\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_2M, label='shifted by 2 month', ax=ax, color='blue')\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal_shift_3M, label='shifted by 3 month', ax=ax, color='navy')\n\nax.set_title('Shifted Time Series', fontsize=16)\n\nax.set_xlim([date(2017, 6, 30), date(2020, 6, 30)])\nax.set_ylabel(ylabel='Temperature Bastia Umbra', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLet's begin by plotting the seasonal components of each feature and comparing the minima and maxima. By doing this, we can already gain some insights:\n* The depth to groundwater reaches its maximum around May/June and its minimum around November/December\n* The temperature reaches its maxmium around August and its minimum around January\n* The volume reaches its maximum around June and its minimum around August/September. It takes longer to reach its maximum than to reach its minimum.\n* The hydrometry reaches its maximum around March and its minimum around September\n\n* The volume and hydrometry reach their minimum roughly around the same time\n* The volume and hydrometry reach their minimum when the temperature reaches its maximum\n* Temperature lags begind depth to groundwater by around 2 to 3 months","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 12))\nf.suptitle('Seasonal Components of Features', fontsize=16)\nsns.lineplot(x=df.Date, y=df.Depth_to_Groundwater_seasonal, ax=ax[0], color='dodgerblue', label='P25')\nax[0].set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Temperature_seasonal, ax=ax[1], color='dodgerblue', label='Bastia Umbra')\nax[1].set_ylabel(ylabel='Temperature', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Drainage_Volume_seasonal, ax=ax[2], color='dodgerblue')\nax[2].set_ylabel(ylabel='Volume', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.River_Hydrometry_seasonal, ax=ax[3], color='dodgerblue')\nax[3].set_ylabel(ylabel='Hydrometry', fontsize=14)\n\nsns.lineplot(x=df.Date, y=df.Rainfall_seasonal, ax=ax[4], color='dodgerblue')\nax[4].set_ylabel(ylabel='Rainfall', fontsize=14)\n\nfor i in range(5):\n    ax[i].set_xlim([date(2017, 9, 30), date(2020, 6, 30)])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the correlation to the target variables increases if we use the time shifted features in comparison to the original features.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\noriginal_cols = ['Depth_to_Groundwater_seasonal', \n                 'Temperature_seasonal',\n                 'Drainage_Volume_seasonal', 'River_Hydrometry_seasonal']\n\ncorrmat = df[original_cols].corr()\n\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[0])\nax[0].set_title('Correlation Matrix of Original Features', fontsize=16)\n\nshifted_cols = [ 'Depth_to_Groundwater_seasonal', \n                'Temperature_seasonal_shift_r_2M',\n                'Drainage_Volume_seasonal_shift_1M', 'River_Hydrometry_seasonal_shift_3M']\ncorrmat = df[shifted_cols].corr()\n\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[1])\nax[1].set_title('Correlation Matrix of Shifted Features', fontsize=16)\n\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autocorrelation Analysis\n\n<div class=\"alert alert-block alert-info\">\n    For further details on this topic, see my other notebook: \n    <a href=\"https://www.kaggle.com/iamleonie/time-series-interpreting-acf-and-pacf\">Time Series: Interpreting ACF and PACF</a>\n</div>\n\n\n\nThis EDA step is especially important when using [ARIMA](#ARIMA). The autocorrelation analysis helps to identify the AR and MA parameters for the [ARIMA](#ARIMA) model.\n\nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF)\n\n* **Autocorrelation  Function (ACF)**: Correlation between time series with a lagged version of itself. The autocorrelation function starts a lag 0, which is the correlation of the time series with itself and therefore results in a correlation of 1. -> <font color='blue'>MA parameter is q significant lags</font>\n* **Partial Autocorrelation Function (PACF)**: Additional correlation explained by each successive lagged term -> <font color='purple'>AR parameter is p significant lags</font>\n\nAutocorrelation helps in detecting seasonality.\n\nAs we can infer from the graph above, the autocorrelation continues to decrease as the lag increases, confirming that there is no linear association between observations separated by larger lags.\n\nFor the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend.","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df.Depth_to_Groundwater_diff_1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see some sinusoidal shape in both ACF and PACF functions. This suggests that both AR and MA processes are present.","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n\nplot_acf(df.Depth_to_Groundwater_diff_1,lags=100, ax=ax[0])\nplot_pacf(df.Depth_to_Groundwater_diff_1,lags=100, ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n## Spectral Analysis\nto analyse cyclic behavior\nFrequency domain analysis\n\n## Trend estimation and decomposition\nused for seasonal adjustment\n\"\"\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation\n\nFor cross validation, you can use the [Time Series Split](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split) library. \nIn [Time Series Forecasting: Building Intuition](https://www.kaggle.com/iamleonie/time-series-forecasting-building-intuition), I go into depth about different types of time series problems and their cross validation methods.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\n\nN_SPLITS = 3\n\nX = df.Date\ny = df.Depth_to_Groundwater\n\nfolds = TimeSeriesSplit(n_splits=N_SPLITS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=N_SPLITS, ncols=2, figsize=(16, 9))\n\nfor i, (train_index, valid_index) in enumerate(folds.split(X)):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    sns.lineplot(x= X_train, y= y_train, ax=ax[i,0], color='dodgerblue', label='train')\n    sns.lineplot(x= X_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n                 y= y_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n                 ax=ax[i,1], color='dodgerblue', label='train')\n\n    for j in range(2):\n        sns.lineplot(x= X_valid, y= y_valid, ax=ax[i, j], color='darkorange', label='validation')\n    ax[i, 0].set_title(f\"Rolling Window with Adjusting Training Size (Split {i+1})\", fontsize=16)\n    ax[i, 1].set_title(f\"Rolling Window with Constant Training Size (Split {i+1})\", fontsize=16)\n\nfor i in range(N_SPLITS):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\n\nTime series can be either **univariate or multivariate**:\n* **Univariate** time series only has a single time-dependent variable.\n* **Multivariate** time series have a multiple time-dependent variable.\n\nOur example originally is a multivariate time series because its has multiple features that are all time-dependent. However, by only looking at the target variable `Depth to Groundwater` we can convert it to a univariate time series.\n\nWe will focus on a **quarterly forecast**. We will use the **Q2 2020 as test data** and the remaining data will be **split by quarter for cross validation**.\n\nWe will evaluate the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE) of the models. For metrics are better the smaller they are.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Models for Univariate Time Series\n\n* Stochastic Models\n    * [Naive Approach](#Naive-Approach)<br>\n    * [Moving Average](#Moving-Average)<br>\n    * [Exponential Smoothing](#MExponential-Smoothing)<br>\n    * [ARIMA](#ARIMA)<br>\n    * [Prophet](#Prophet)<br>\n* Deep Learning\n    * [LSTM](#LSTM)<br>\n    * [GRU](#GRU)<br>","metadata":{}},{"cell_type":"code","source":"df['quarter_idx'] = (df.quarter != df.quarter.shift(1)).cumsum()\n\ntarget = 'Depth_to_Groundwater'\nfeatures = [feature for feature in df.columns if feature != target]\n\nN_SPLITS = 46\n\nX = df[df.quarter_idx < N_SPLITS][features]\ny = df[df.quarter_idx < N_SPLITS][target]\n\nX_test = df[df.quarter_idx == N_SPLITS][features].reset_index(drop=True)\ny_test = df[df.quarter_idx == N_SPLITS][target].reset_index(drop=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = np.linspace(0, N_SPLITS-3, num=N_SPLITS-2)\n\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n\nsns.lineplot(x=X.Date, y=y, ax=ax[0], color='dodgerblue', label='train')\nsns.lineplot(x=X_test.Date, y=y_test, ax=ax[0], color='darkorange', label='test')\n\nsns.lineplot(x=df.Date, y=df.quarter_idx, ax=ax[1], color='dodgerblue')\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nax[1].set_ylim([0, N_SPLITS+1])\n#ax[0].set_ylim([-28, -23])\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_approach_evaluation(y_pred, score_mae, score_rsme, approach_name):\n    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n    f.suptitle(approach_name, fontsize=16)\n    sns.lineplot(x=X.Date, y=y, ax=ax[0], color='dodgerblue', label='Training', linewidth=2)\n    sns.lineplot(x=X_test.Date, y=y_test, ax=ax[0], color='gold', label='Ground Truth', linewidth=2) #navajowhite\n    sns.lineplot(x=X_test.Date, y=y_pred, ax=ax[0], color='darkorange', label='Predicted', linewidth=2)\n    ax[0].set_xlim([date(2018, 6, 30), date(2020, 6, 30)])\n    ax[0].set_ylim([-27, -23])\n    ax[0].set_title(f'Prediction \\n MAE: {mean_absolute_error(y_test, y_pred):.2f}, RSME: {math.sqrt(mean_squared_error(y_valid, y_valid_pred)):.2f}', fontsize=14)\n    ax[0].set_xlabel(xlabel='Date', fontsize=14)\n    ax[0].set_ylabel(ylabel='Depth to Groundwater P25', fontsize=14)\n\n    sns.lineplot(x=folds, y=score_mae,  color='gold', label='MAE', ax=ax[1])#marker='o',\n    sns.lineplot(x=folds, y=score_rsme, color='indianred', label='RSME', ax=ax[1])\n    ax[1].set_title('Loss', fontsize=14)\n    ax[1].set_xlabel(xlabel='Fold', fontsize=14)\n    ax[1].set_ylabel(ylabel='Loss', fontsize=14)\n    ax[1].set_ylim([0, 4])   \n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Approach\n\n$\\hat y_{t+1} = y_t$","metadata":{}},{"cell_type":"code","source":"score_mae = []\nscore_rsme = []\nfor fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n    # Get indices for this fold\n    train_index = df[df.quarter_idx < valid_quarter_id].index\n    valid_index = df[df.quarter_idx == valid_quarter_id].index\n\n    # Prepare training and validation data for this fold\n    #X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    # Initialize y_valid_pred\n    y_valid_pred = pd.Series(np.ones(len(y_valid)))\n    \n    # Prediction: Naive approach\n    y_valid_pred = y_valid_pred * y_train.iloc[-1]\n    \n    # Calcuate metrics\n    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n\ny_pred = pd.Series(np.ones(len(X_test))) * y.iloc[-1]\n\nplot_approach_evaluation(y_pred, score_mae, score_rsme, 'Naive Approach')","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Moving Average","metadata":{}},{"cell_type":"code","source":"score_mae = []\nscore_rsme = []\nfor fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n    # Get indices for this fold\n    train_index = df[df.quarter_idx < valid_quarter_id].index\n    valid_index = df[df.quarter_idx == valid_quarter_id].index\n\n    # Prepare training and validation data for this fold\n    #X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    # Initialize y_valid_pred\n    y_valid_pred = pd.Series(np.ones(len(y_valid)))\n    \n    # Prediction: Naive approach    \n    for i in range(len(y_valid_pred)):\n        y_valid_pred.iloc[i] = y_train.append(y_valid_pred.iloc[:(i)]).reset_index(drop=True).rolling(4).mean().iloc[-1]\n        \n    # Calcuate metrics\n    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n\ny_pred = pd.Series(np.zeros(len(X_test)))\n\nfor i in range(len(y_pred)):\n    y_pred.iloc[i] = y.append(y_pred.iloc[:(i)]).reset_index(drop=True).rolling(4).mean().iloc[-1]\n\nplot_approach_evaluation(y_pred, score_mae, score_rsme, 'Moving Average (Window = 4 Weeks)')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neither the Naive Approach nor the Moving Average Approach are yielding good results for our example. Usually, these approaches serve as a benchmark rather than the method of choice.","metadata":{}},{"cell_type":"markdown","source":"### ARIMA\nThe Auto-Regressive Integrated Moving Average (ARIMA) model describes the **autocorrelations** in the data. The model assumes that the time-series is **stationary**. It consists of three main parts:\n* <font color='purple'>Auto-Regressive (AR) filter (long term)</font>: \n    \n    $\\color{purple}{y_t = c + \\alpha_1 y_{t-1} + \\dots \\alpha_{\\color{purple}p}y_{t-\\color{purple}p} + \\epsilon_t = c + \\sum_{i=1}^p{\\alpha_i}y_{t-i} + \\epsilon_t}$  -> p\n* <font color='orange'> Integration filter (stochastic trend)</font>\n    \n    -> d\n* <font color='blue'>Moving Average (MA) filter (short term)</font>:\n\n    $\\color{blue}{y_t = c + \\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q} = c + \\epsilon_t + \\sum_{i=1}^q{\\beta_i}\\epsilon_{t-i}} $  -> q \n\n\n**ARIMA**: $y_t = c + \\color{purple}{\\alpha_1 y_{t-1} + \\dots + \\alpha_{\\color{purple}p}y_{t-\\color{purple}p}} \n+ \\color{blue}{\\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q}}$\n\n\nARIMA(\n<font color='purple'>p</font>,\n<font color='orange'>d</font>,\n<font color='blue'>q</font>)\n\n* <font color='purple'>p</font>: Lag order (to determine see  PACF in [Autocorrelation Analysis](#Autocorrelation-Analysis))\n* <font color='orange'>d</font>: Degree of differencing. (to determine see  Differencing in [Stationarity](#Stationarity))\n* <font color='blue'>q</font>: Order of moving average (to determine see  ACF in [Autocorrelation Analysis](#Autocorrelation-Analysis))\n\nIn our example, we can use <font color='orange'>d=0</font> if we use the feature `Depth_to_Groundwater_diff_1`, which is `Depth_to_Groundwater` differenced by the first degree. Otherwise, if we were to use the non-stationary feature `Depth_to_Groundwater` as it is, we should set <font color='orange'>d=1</font>.\n\n(work in progress...)","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\n\nscore_mae = []\nscore_rsme = []\n\nfor fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n    # Get indices for this fold\n    train_index = df[df.quarter_idx < valid_quarter_id].index\n    valid_index = df[df.quarter_idx == valid_quarter_id].index\n\n    # Prepare training and validation data for this fold\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    # Fit model with Vector Auto Regression (VAR)\n    model = ARIMA(y_train, order=(1,1,1))\n    model_fit = model.fit()\n    \n    # Prediction with Vector Auto Regression (VAR)\n    y_valid_pred = model_fit.predict(valid_index[0], valid_index[-1])\n\n    # Calcuate metrics\n    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n\n\n# Fit model with Vector Auto Regression (VAR)\nmodel = ARIMA(y, order=(1,1,1))\nmodel_fit = model.fit()\n\n# Prediction with Vector Auto Regression (VAR)\ny_pred = model_fit.predict(y.index[-1]+1, y.index[-1] + len(y_test)).reset_index(drop=True)\nplot_approach_evaluation(y_pred, score_mae, score_rsme, 'ARIMA')","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models for Multivariate Time Series\n\n### Vector Auto Regression (VAR)","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.api import VAR\n\nscore_mae = []\nscore_rsme = []\n\nfeatures = ['Temperature', 'Drainage_Volume', 'River_Hydrometry','Rainfall' ]\nfor fold, valid_quarter_id in enumerate(range(2, N_SPLITS)):\n    # Get indices for this fold\n    train_index = df[df.quarter_idx < valid_quarter_id].index\n    valid_index = df[df.quarter_idx == valid_quarter_id].index\n\n    # Prepare training and validation data for this fold\n    X_train, X_valid = X.iloc[train_index][features], X.iloc[valid_index][features]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    # Fit model with Vector Auto Regression (VAR)\n    model = VAR(pd.concat([y_train, X_train], axis=1))\n    model_fit = model.fit()\n    \n    # Prediction with Vector Auto Regression (VAR)\n    y_valid_pred = model_fit.forecast(model_fit.y, steps=len(X_valid))\n    y_valid_pred = pd.Series(y_valid_pred[:, 0])\n\n    # Calcuate metrics\n    score_mae.append(mean_absolute_error(y_valid, y_valid_pred))\n    score_rsme.append(math.sqrt(mean_squared_error(y_valid, y_valid_pred)))\n\n# Fit model with Vector Auto Regression (VAR)\nmodel = VAR(pd.concat([y, X[features]], axis=1))\nmodel_fit = model.fit()\n\n# Prediction with Vector Auto Regression (VAR)\ny_pred = model_fit.forecast(model_fit.y, steps=len(X_valid))\ny_pred = pd.Series(y_pred[:, 0])\n\nplot_approach_evaluation(y_pred, score_mae, score_rsme, 'Vector Auto Regression (VAR)')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Ressources\n\n## My Time Series Forecasting Series\n\n* [Intro to Time Series Forecasting](https://www.kaggle.com/iamleonie/intro-to-time-series-forecasting)\n* [Time Series Forecasting: Building Intuition](https://www.kaggle.com/iamleonie/time-series-forecasting-building-intuition)\n* [Time Series Forecasting: Interpreting ACF and PACF](https://www.kaggle.com/iamleonie/time-series-interpreting-acf-and-pacf)\n* [Time Series Forecasting: Tips & Tricks for Training LSTMs](https://www.kaggle.com/iamleonie/time-series-tips-tricks-for-training-lstms)\n\n## Other Ressources\nHere are some additional ressources that helped me learn about time series\n* [Getting started with Time Series using Pandas ](https://www.kaggle.com/parulpandey/getting-started-with-time-series-using-pandas)\n* [Time Series Analysis || An Introductory Start](https://www.kaggle.com/janiobachmann/time-series-analysis-an-introductory-start)\n* [Everything you can do with a time series](https://www.kaggle.com/thebrownviking20/everything-you-can-do-with-a-time-series)\n* [Deep Learning for Time Series | Dimitry Larko | Kaggle Days](https://www.youtube.com/watch?v=svNwWSgz2NM)\n* [Encoding Cyclical Features for Deep Learning](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* [Tamara Louie: Applying Statistical Modeling & Machine Learning to Perform Time-Series Forecasting](https://www.youtube.com/watch?v=JntA9XaTebs)\n* [Forecasting: Principles and Practice](https://otexts.com/fpp2/)\n* [Easy Guide on Time Series Forecasting](https://beingdatum.com/time-series-forecasting/)\n\n# Useful Libraries\n* [statsmodels](https://www.statsmodels.org)\n* [Pandas Time series / date functionality](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html)\n* [Time Series Split](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)","metadata":{}},{"cell_type":"code","source":"\"\"\"\n### Exponential Smoothing\nbased on a description of the **trend and seasonality** in the data\n\n### Prophet \n\n### LSTM\n\n### GRU\n\n(work in progress)\n\nLong Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\n\nRecurrent Neural Network (RNN)\n\"\"\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}